<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1BCM7V96NQ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-1BCM7V96NQ');
    </script>
    <title>Yusong Wu</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--  <link rel="icon" href="favicon.png" type="image/x-icon">-->
    <link rel="stylesheet" href="css/main.bundle.css">
    <script src="https://kit.fontawesome.com/4e5a72c756.js"></script>
</head>

<body>
<nav class="navbar is-primary" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <div class="navbar-item is-hidden-desktop">
            <a href="https://github.com/lukewys" class="icon is-large has-text-light">
                <i class="fab fa-2x fa-github"></i>
            </a>
        </div>
    </div>
    <div class="navbar-menu">
        <div class="navbar-end">
            <div class="navbar-item">
                <a href="https://github.com/lukewys" class="icon is-large has-text-light">
                    <i class="fab fa-2x fa-github"></i>
                </a>
            </div>
        </div>
    </div>
</nav>

<section class="hero is-primary">
    <div class="hero-body">
        <div class="container">
            <div class="level">
                <div class="level-item has-text-centered">
                    <figure class="image is-128x128">
                        <img class="is-rounded" src="img/profile.jpeg">
                    </figure>
                </div>
            </div>
            <div class="columns has-text-centered">
                <div class="column">
                    <h1 class="title is-1">
                        Yusong Wu (吴雨松)
                    </h1>
                    <h2 class="subtitle">
                        2nd year research master at Mila, University of Montreal. <br> Working on music generation.
                    </h2>
                    <p>
                        <a href="files/cv_yusongwu.pdf" class="icon has-text-light">
                            <i class="fas fa-file-word"></i> <strong>Resume</strong>
                        </a>
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container">
        <h1 class="title">About Me</h1>
        <hr/>
        <p> I am a first year PHD student of computer science at the University of
            Montreal and Mila.
            I am fortunate to be co-advised by
            <a href="https://mila.quebec/en/person/aaron-courville/">Prof. Aaron Courville</a>
            and <a href="https://mila.quebec/en/person/anna-huang/">Prof. Chengzhi Anna Huang</a>.
            My research interest is mainly on music generation, including audio and symbolic music.
            I am also interested in multi-modality learning with music and audio.
            I am a percussion player, and I used to play timpani in the orchestra.
            I will also play guitar and harmonica in my spare time.
            <!--            I am currently looking for a PhD position.-->
        </p>
    </div>
</section>

<section class="section">
    <div class="container">
        <h1 class="title">Selected Publications and Manuscripts</h1>
        <hr/>
        <div class="content">
            <ul>
                <li><b>Yusong Wu</b>, Ethan Manilow, Yi Deng, Rigel Swavely, Kyle Kastner, Tim Cooijmans,
                    Aaron Courville, Cheng-Zhi Anna Huang, Jesse Engel:
                    <i>MIDI-DDSP: detailed control of musical performance via hierarchical modeling</i>.
                    <b><a href="https://openreview.net/forum?id=UseMOjWENv">ICLR 2022 oral</a></b>,
                    outstanding paper award of CtrlGen Workshop at NeurIPS 2021.
                </li>
                <li><b>Yusong Wu</b>, Kun Chen, Ziyue Wang, Xuan Zhang, Fudong Nian, Xi Shao, Shengchen Li:
                    <i>Audio Captioning Based on Transformer and Pre-Training for 2020 DCASE Audio Captioning
                        Challenge</i>.
                    Technical Report, DCASE2020 Challenge (2nd place in the challenge and Reproducible System Award)
                </li>
                <li><b>Yusong Wu</b>, Shengchen Li, Chenzhu Yu, Heng Lu, Chao Weng, Dong Yu:
                    <i>Peking Opera Synthesis via Duration Informed Attention Network</i>.
                    INTERSPEECH 2020
                </li>
                <li>Liqiang Zhang, Chengzhu Yu, Heng Lu, Chao Weng, <b>Yusong Wu</b>, Xiang Xie, Zijin Li, Dong Yu:
                    <i>DurIAN-SC: Duration Informed Attention Network based Singing Voice Conversion System</i>.
                    INTERSPEECH 2020
                </li>
                <li>Xinhao Mei, Qiushi Huang, Xubo Liu, Gengyun Chen, Jingqian Wu, <b>Yusong Wu</b>, Jinzheng Zhao,
                    Shengchen Li,
                    Tom Ko, H Lilian Tang, Xi Shao, Mark D Plumbley, Wenwu Wang:
                    <i>An encoder-decoder based audio captioning system with transfer and
                        reinforcement learning for DCASE challenge 2021 task 6</i> Technical Report, DCASE2021 Challenge
                    (3rd place in the challenge)
                </li>
                <li><b>Yusong Wu</b>, Shengchen Li: Guqin Dataset:
                    <i>A symbolic music dataset of Chinese Guqin collection</i>.
                    Proceedings of China Conference on Sound and Music Technology (CSMT 2019)
                </li>
                <li><b>Yusong Wu</b>, Shengchen Li: <i>Distinguishing Chinese Guqin and Western Baroque pieces based on
                    statistical
                    model analysis of melodies</i>.
                    International Symposium on Computer Music Multidisciplinary Research (CMMR 2019)
                </li>
            </ul>
        </div>
    </div>
</section>

<section class="section">
    <div class="container">
        <h1 class="title">Projects</h1>
        <hr/>
        <div class="tile is-ancestor">
            <div class="tile is-vertical is-12">
                <div class="tile">
                    <div class="tile is-parent">
                        <article class="tile is-child notification">
                            <p class="title">Hierarchical Music Generation with Detailed Control</p>
                            <figure class="image is-4by3">
                                <img class="modal-trigger" data-target="project-1-modal" src="img/midi-ddsp.png">
                            </figure>
                        </article>
                    </div>
                    <div class="tile is-parent">
                        <article class="tile is-child notification">
                            <p class="title">Automatic Audio Captioning with Transformer</p>
                            <figure class="image is-4by3">
                                <img class="modal-trigger" data-target="project-2-modal" src="img/dcase.png">
                            </figure>
                        </article>
                    </div>
                </div>
            </div>
        </div>
        <div class="tile is-ancestor">
            <div class="tile is-vertical is-12">
                <div class="tile">
                    <div class="tile is-parent">
                        <article class="tile is-child notification">
                            <p class="title">Expressive Peking Opera Synthesis</p>
                            <figure class="image is-4by3">
                                <img class="modal-trigger" data-target="project-3-modal" src="img/peking-opera.png">
                            </figure>
                        </article>
                    </div>
                    <div class="tile is-parent">
                        <article class="tile is-child notification">
                            <p class="title">Chinese Guqin Dataset <br> <br></p>
                            <figure class="image is-4by3">
                                <img class="modal-trigger" data-target="project-4-modal" src="img/guqin.png">
                            </figure>
                        </article>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container">
        <h1 class="title">Experience</h1>
        <hr/>
        <div class="card">
            <div class="card-content">
                <div class="media">
                    <div class="media-left">
                        <figure class="image is-48x48">
                            <img src="img/mila-square.png" alt="Placeholder image">
                        </figure>
                    </div>
                    <div class="media-content">
                        <p class="title is-4">Mila, University of Montreal</p>
                        <p class="subtitle is-6">CS Research Msc -
                            <time datetime="2020-9">September 2020</time>
                            -
                            <time
                                    datetime="2022-6">Now
                            </time>
                        </p>
                    </div>
                </div>
                <div class="content">
                    <p>Work on music generation.</p>
                    <ul>
                        <li>Propose <a href="https://midi-ddsp.github.io/">MIDI-DDSP</a>, a hierarchical music
                            generation model
                            with explicit and interpretable representation for controlling musical performance and
                            synthesis.
                        </li>
                        <li>MIDI-DDSP can reconstruct high-fidelity audio,
                            accurately predict performance attributes for a note sequence,
                            independently manipulate the attributes of a given performance,
                            and as a complete system, generate realistic audio from a novel note sequence.
                        </li>
                    </ul>
                    <div class="tags">
                        <span class="tag">Audio Music Generation</span>
                        <span class="tag">Symbolic Music Generation</span>
                    </div>
                </div>
            </div>
        </div>
        <div class="card card-gap">
            <div class="card-content">
                <div class="media">
                    <div class="media-left">
                        <figure class="image is-48x48">
                            <img src="img/tencent-ai-lab.png" alt="Placeholder image">
                        </figure>
                    </div>
                    <div class="media-content">
                        <p class="title is-4">Tencent AI Lab</p>
                        <p class="subtitle is-6">Research Intern -
                            <time datetime="2019-8">August 2019</time>
                            -
                            <time
                                    datetime="2020-5">May 2020
                            </time>
                        </p>
                    </div>
                </div>
                <div class="content">
                    <p>Working on singing voice synthesis.</p>
                    <ul>
                        <li><a href="https://lukewys.github.io/files/Peking-Opera-Synthesis-2020.html">
                            Expressive Singing Performance</a>:
                            Experimented synthesizing Peking Opera singing with expressiveness in singing by
                            inputting musical note, with the dynamics in Peking opera singing learned from the
                            spectrogram.
                        </li>
                        <li><a href="https://tencent-ailab.github.io/learning_singing_from_speech/">
                            Learning Singing from Speech</a>:
                            Experimented generating singing with the voice timbre learned from speech by jointly
                            training singing and fine-tuning speech synthesis using fundamental frequency input.
                        </li>
                    </ul>
                    <div class="tags">
                        <span class="tag">Singing Voice Synthesis</span>
                    </div>
                </div>
            </div>
        </div>
        <div class="card card-gap">
            <div class="card-content">
                <div class="media">
                    <div class="media-left">
                        <figure class="image is-48x48">
                            <img src="img/bupt.png" alt="Placeholder image">
                        </figure>
                    </div>
                    <div class="media-content">
                        <p class="title is-4">Beijing University of Posts and Telecommunications</p>
                        <p class="subtitle is-6">
                            <time datetime="2016-9">September 2016</time>
                            -
                            <time
                                    datetime="2020-6">June 2020
                            </time>
                        </p>
                    </div>
                </div>
                <div class="content">
                    <p>Received Bachelor of Engineer in Automation. Doing research advised by
                        <a href="https://www.xjtlu.edu.cn/en/departments/academic-departments/intelligent-science/staff/shengchen-li">
                            Prof. Shengchen Li</a>.</p>
                    <ul>
                        <li>Working on DCASE 2020 Challenge of Automatic Audio Captioning.
                            Won
                            <a href="http://dcase.community/challenge2020/task-automatic-audio-captioning-results#wuyusong2020_t6">2nd
                                place</a>
                            of challenge and Reproducible System Award.
                        </li>
                        <li>Collect and construct a symbolic music dataset of
                            <a href="https://github.com/lukewys/Guqin-Dataset">Chinese Guqin Dataset</a>.
                        </li>
                        <li>Work on computational musicology, proposed statistical approach to distinguishing different
                            music genre.
                        </li>
                    </ul>
                    <div class="tags">
                        <span class="tag">Music Generation</span>
                        <span class="tag">Automatic Audio Captioning</span>
                        <span class="tag">Computational Musicology</span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container">
        <h1 class="title">Contact</h1>
        <hr/>
        <nav class="level">
            <div class="level-item has-text-centered">
                <div>
                    <a href="https://github.com/lukewys" class="icon is-large">
                        <i class="fab fa-2x fa-github"></i>
                    </a>
                    <p class="heading"><a href="https://github.com/lukewys">github.com/lukewys</a></p>
                </div>
            </div>
            <div class="level-item has-text-centered">
                <div>
                    <a href="https://scholar.google.com/citations?user=vIuUJ-IAAAAJ&hl" class="icon is-large">
                        <i class="fas fa-2x fa-globe"></i>
                    </a>
                    <p class="heading"><a href="https://scholar.google.com/citations?user=vIuUJ-IAAAAJ&hl">Google
                        Scholar</a></p>
                </div>
            </div>
            <div class="level-item has-text-centered">
                <div>
                    <a href="mailto:wuyusongwys@gmail.com" class="icon is-large">
                        <i class="fas fa-2x fa-envelope"></i>
                    </a>
                    <p class="heading"><a href="mailto:wuyusongwys@gmail.com">wuyusongwys@gmail.com</a></p>
                </div>
            </div>
            <div class="level-item has-text-centered">
                <div>
                    <a href="https://www.linkedin.com/in/%E9%9B%A8%E6%9D%BE-%E5%90%B4-86b639187/" class="icon is-large">
                        <i class="fab fa-2x fa-linkedin-in"></i>
                    </a>
                    <p class="heading"><a href="https://www.linkedin.com/in/%E9%9B%A8%E6%9D%BE-%E5%90%B4-86b639187/">linkedin</a>
                    </p>
                </div>
            </div>
        </nav>
    </div>
</section>

<!-- Modals -->
<div id="project-1-modal" class="modal">
    <div class="modal-background"></div>
    <div class="modal-card">
        <header class="modal-card-head">
            <p class="modal-card-title">Hierarchical Music Generation with Detailed Control</p>
            <button class="delete" aria-label="close"></button>
        </header>
        <section class="modal-card-body">

            <div id="project-1-modal-carousel" class="carousel">
                <figure class="image carousel-cell">
                    <img src="img/midi-ddsp-ori-scale.png">
                </figure>
                <figure class="image carousel-cell">
                    <img src="img/midi-ddsp-listening-test.png">
                </figure>
                <figure class="image carousel-cell">
                    <img src="img/midi-ddsp-control-metric.png">
                </figure>
            </div>

            <div class="content">

                <li>Propose <a href="https://midi-ddsp.github.io/">MIDI-DDSP</a>, a hierarchical music generation model
                    with explicit and interpretable representation for controlling musical performance and synthesis.
                </li>
                <li>MIDI-DDSP can reconstruct high-fidelity audio,
                    accurately predict performance attributes for a note sequence,
                    independently manipulate the attributes of a given performance,
                    and as a complete system, generate realistic audio from a novel note sequence.
                </li>
                <p><a href="https://github.com/magenta/midi-ddsp">Code</a> |
                    <a href="https://magenta.tensorflow.org/midi-ddsp">Blog</a> |
                    <a href="https://colab.research.google.com/github/magenta/midi-ddsp/blob/main/midi_ddsp/colab/MIDI_DDSP_Demo.ipynb">Colab
                        Notebook</a> |
                    <a href="https://huggingface.co/spaces/akhaliq/midi-ddsp">Huggingface Space</a> |
                    <a href="https://github.com/magenta/midi-ddsp#command-line-midi-synthesis">Command-line MIDI
                        Synthesis</a></p>


                <div class="tags">
                    <span class="tag">Audio Synthesis</span>
                    <span class="tag">Generative Model</span>
                    <span class="tag">Hierarchical, DDSP</span>
                    <span class="tag">Structured Models</span>
                </div>

            </div>

        </section>
        <footer class="modal-card-foot">
            <button class="button is-success">Close</button>
        </footer>
    </div>
</div>

<div id="project-2-modal" class="modal">
    <div class="modal-background"></div>
    <div class="modal-card">
        <header class="modal-card-head">
            <p class="modal-card-title">Automatic Audio Captioning with Transformer</p>
            <button class="delete" aria-label="close"></button>
        </header>
        <section class="modal-card-body">

            <div id="project-2-modal-carousel" class="carousel">
                <figure class="image carousel-cell">
                    <img src="img/dcase-ori-scale.png">
                </figure>
            </div>

            <div class="content">

                <li>Proposed a sequence-to-sequence model with a CNN as encoder and a Transformer as decoder,
                    with data augmentation, data regulation, pre-training and fine-tuning for accurate automatic audio
                    captioning.
                </li>
                <li>The proposed system ranked
                    <a href="http://dcase.community/challenge2020/task-automatic-audio-captioning-results#wuyusong2020_t6">2nd</a>
                    in all participants achieved by a SPIDEr score of 0.214.
                </li>
                <li>The proposed system won Reproducible System Award.</li>
                <p><a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf">paper</a>
                </p>


                <div class="tags">
                    <span class="tag">Automatic Audio Captioning</span>
                    <span class="tag">Transformer</span>
                    <span class="tag">Seq-2-Seq Model</span>
                </div>

            </div>

        </section>
        <footer class="modal-card-foot">
            <button class="button is-success">Close</button>
        </footer>
    </div>
</div>

<div id="project-3-modal" class="modal">
    <div class="modal-background"></div>
    <div class="modal-card">
        <header class="modal-card-head">
            <p class="modal-card-title">Expressive Peking Opera Synthesis</p>
            <button class="delete" aria-label="close"></button>
        </header>
        <section class="modal-card-body">
            <div id="project-3-modal-carousel" class="carousel">
                <figure class="image is-16by9 carousel-cell">
                    <img src="img/peking-opera-synthesis.png">
                </figure>
            </div>

            <div class="content">
                <p>Propose a Peking Opera singing synthesis system with expressiveness and dynamics in Peking opera
                    singing.
                    The model is trained by inputting musical note and learned from the spectrogram.</p>
                <p><a href="https://lukewys.github.io/files/Peking-Opera-Synthesis-2020.html">demo</a>
                    <a href="https://lukewys.github.io/files/INTERSPEECH_2020_YusongWu_Final_revised.pdf">paper</a></p>
                <div class="tags">
                    <span class="tag">Singing Voice Synthesis</span>
                </div>
            </div>

        </section>
        <footer class="modal-card-foot">
            <button class="button is-success">Close</button>
        </footer>
    </div>
</div>

<div id="project-4-modal" class="modal">
    <div class="modal-background"></div>
    <div class="modal-card">
        <header class="modal-card-head">
            <p class="modal-card-title">Chinese Guqin Dataset</p>
            <button class="delete" aria-label="close"></button>
        </header>
        <section class="modal-card-body">
            <div id="project-4-modal-carousel" class="carousel">
                <figure class="image carousel-cell">
                    <img src="img/guqin.jpg">
                </figure>
            </div>

            <div class="content">
                <p>Collect and construct a symbolic music dataset of
                    <a href="https://github.com/lukewys/Guqin-Dataset">Chinese Guqin Dataset</a>.
                    Dataset is also used for proposing a statistical approach to distinguishing different music genre.
                </p>
                <div class="tags">
                    <span class="tag">Computational Musicology</span>
                </div>
            </div>

        </section>
        <footer class="modal-card-foot">
            <button class="button is-success">Close</button>
        </footer>
    </div>
</div>


<script src="js/bundle.js"></script>
</body>

</html>