<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1BCM7V96NQ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-1BCM7V96NQ');
    </script>
    <title>Yusong Wu</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--  <link rel="icon" href="favicon.png" type="image/x-icon">-->
    <link rel="stylesheet" href="css/main.bundle.css">
    <script src="https://kit.fontawesome.com/4e5a72c756.js"></script>
</head>

<body>
    <nav class="navbar is-primary" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <div class="navbar-item is-hidden-desktop">
                <a href="https://github.com/lukewys" class="icon is-large has-text-light">
                    <i class="fab fa-2x fa-github"></i>
                </a>
            </div>
        </div>
        <div class="navbar-menu">
            <div class="navbar-end">
                <div class="navbar-item">
                    <a href="https://github.com/lukewys" class="icon is-large has-text-light">
                        <i class="fab fa-2x fa-github"></i>
                    </a>
                </div>
            </div>
        </div>
    </nav>

    <section class="hero is-primary">
        <div class="hero-body">
            <div class="container">
                <div class="level">
                    <div class="level-item has-text-centered">
                        <figure class="image is-128x128">
                            <img class="is-rounded" src="img/profile.jpeg">
                        </figure>
                    </div>
                </div>
                <div class="columns has-text-centered">
                    <div class="column">
                        <h1 class="title is-1">
                            Yusong Wu (吴雨松)
                        </h1>
                        <h2 class="subtitle">
                            Third-year PhD candidate at Mila, University of Montreal, focusing on interactive music
                            generation.
                        </h2>
                        <div class="is-flex is-justify-content-center is-align-items-center">
                            <p class="mr-6">
                                <a href="files/cv_yusongwu.pdf" class="icon has-text-light">
                                    <i class="fas fa-file-word"></i> <strong>Resume</strong>
                                </a>
                            </p>
                            <p class="ml-6">
                                <a href="https://scholar.google.com/citations?user=vIuUJ-IAAAAJ&hl"
                                    class="icon has-text-light" style="white-space: nowrap;">
                                    <i class="fas fa-globe"></i> <strong>Google Scholar</strong>
                                </a>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container">
            <h1 class="title">About Me</h1>
            <hr />
            <p> I am a third-year PhD candidate in Computer Science at the University of Montreal and Mila.
                I am fortunate to be co-advised by
                <a href="https://mila.quebec/en/person/aaron-courville/">Professor Aaron Courville</a>
                and <a href="https://mila.quebec/en/person/anna-huang/">Professor Chengzhi Anna Huang</a>.
                My doctoral research focuses on developing interactive and creative music generation models.
                I am also interested in multimodal learning approaches integrating music and audio.
                I am a percussionist who has performed timpani in orchestral settings. In my spare time, I also enjoy
                playing the guitar and harmonica.
            </p>
        </div>
    </section>


    <section class="section">
        <div class="container">
            <h1 class="title">Selected Publications and Manuscripts</h1>
            <hr />
            <div class="content">
                <ul>
                    <li><b>Yusong Wu</b>, Tim Cooijmans, Kyle Kastner, Adam Roberts, Ian Simon, Alexander Scarlatos,
                        Chris Donahue, Cassie Tarakajian, Shayegan Omidshafiei, Aaron Courville, Pablo Samuel Castro,
                        Natasha Jaques, Cheng-Zhi Anna Huang:
                        <i>Adaptive Accompaniment with ReaLchords</i>.
                        <b>ICML 2024</b>
                    </li>
                    <li>Ke Chen, <b>Yusong Wu</b>, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo
                        Dubnov:
                        <i>MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup
                            Strategies</i>.
                        <b>ICASSP 2024</b>
                    </li>
                    <li><b>Yusong Wu*</b>, Ke Chen*, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov:
                        <i>Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption
                            Augmentation</i>.
                        <b>ICASSP 2023</b>
                    </li>
                    <li><b>Yusong Wu</b>, Josh Gardner, Ethan Manilow, Ian Simon, Curtis Hawthorne, Jesse Engel:
                        <i>The Chamber Ensemble Generator: Limitless High-Quality MIR Data via Generative Modeling</i>.
                        <b>arXiv preprint</b> arXiv:2209.14458
                    </li>
                    <li><b>Yusong Wu</b>, Ethan Manilow, Yi Deng, Rigel Swavely, Kyle Kastner, Tim Cooijmans, Aaron
                        Courville, Cheng-Zhi Anna Huang, Jesse Engel:
                        <i>MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling</i>.
                        <b>ICLR 2022 oral (5%)</b>
                    </li>
                    <li><b>Yusong Wu</b>, Kun Chen, Ziyue Wang, Xuan Zhang, Fudong Nian, Xi Shao, Shengchen Li:
                        <i>Audio Captioning Based on Transformer and Pre-Training for 2020 DCASE Audio Captioning
                            Challenge</i>.
                        Technical Report, <b>DCASE 2020 Challenge</b> (2nd place in the challenge and Reproducible
                        System Award)
                    </li>
                    <li><b>Yusong Wu</b>, Shengchen Li, Chenzhu Yu, Heng Lu, Chao Weng, Dong Yu:
                        <i>Peking Opera Synthesis via Duration Informed Attention Network</i>.
                        <b>INTERSPEECH 2020</b>
                    </li>
                    <li>Liqiang Zhang, Chengzhu Yu, Heng Lu, Chao Weng, <b>Yusong Wu</b>, Xiang Xie, Zijin Li, Dong Yu:
                        <i>DurIAN-SC: Duration Informed Attention Network based Singing Voice Conversion System</i>.
                        <b>INTERSPEECH 2020</b>
                    </li>
                </ul>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container">
            <h1 class="title">Projects</h1>
            <hr />

            <div class="tile is-ancestor">
                <div class="tile is-vertical is-12">
                    <div class="tile">
                        <div class="tile is-parent">
                            <article class="tile is-child notification">
                                <p class="title">ReaLchords and GenJam: Real-time Melody-to-chord Accompaniment via RL
                                </p>
                                <figure class="image is-16by9">
                                    <img class="modal-trigger" data-target="project-8-modal" src="img/genjam_ui.png">
                                </figure>
                            </article>
                        </div>
                        <div class="tile is-parent">
                            <article class="tile is-child notification">
                                <p class="title">CLAP: Large-scale Contrastive Language-audio Model</p>
                                <figure class="image is-4by3">
                                    <img class="modal-trigger" data-target="project-7-modal" src="img/clap_diagram.png">
                                </figure>
                            </article>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tile is-ancestor">
                <div class="tile is-vertical is-12">
                    <div class="tile">
                        <div class="tile is-parent">
                            <article class="tile is-child notification">
                                <p class="title">MusicLDM: Text-to-Music Generation with Mixup</p>
                                <figure class="image is-16by9">
                                    <img class="modal-trigger" data-target="project-6-modal"
                                        src="img/musicldm_diagram.png">
                                </figure>
                            </article>
                        </div>
                        <div class="tile is-parent">
                            <article class="tile is-child notification">
                                <p class="title">3rd Place at AI Song Contest 2022</p>
                                <figure class="image is-1by1">
                                    <img class="modal-trigger" data-target="project-5-modal"
                                        src="img/aisongcontest_atoi_square.jpeg">
                                </figure>
                            </article>
                        </div>
                        <div class="tile is-parent">
                            <article class="tile is-child notification">
                                <p class="title">Hierarchical Music Generation with Detailed Control</p>
                                <figure class="image is-4by3">
                                    <img class="modal-trigger" data-target="project-1-modal" src="img/midi-ddsp.png">
                                </figure>
                            </article>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tile is-ancestor">
                <div class="tile is-vertical is-12">
                    <div class="tile">
                        <div class="tile is-parent">
                            <article class="tile is-child notification">
                                <p class="title">Automatic Audio Captioning with Transformer</p>
                                <figure class="image is-4by3">
                                    <img class="modal-trigger" data-target="project-2-modal" src="img/dcase.png">
                                </figure>
                            </article>
                        </div>
                        <div class="tile is-parent">
                            <article class="tile is-child notification">
                                <p class="title">Expressive Peking Opera Synthesis</p>
                                <figure class="image is-4by3">
                                    <img class="modal-trigger" data-target="project-3-modal" src="img/peking-opera.png">
                                </figure>
                            </article>
                        </div>
                        <div class="tile is-parent">
                            <article class="tile is-child notification">
                                <p class="title">Chinese Guqin Dataset <br> <br></p>
                                <figure class="image is-4by3">
                                    <img class="modal-trigger" data-target="project-4-modal" src="img/guqin.png">
                                </figure>
                            </article>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container">
            <h1 class="title">Experience</h1>
            <hr />
            <div class="card">
                <div class="card-content">
                    <div class="media">
                        <div class="media-left">
                            <figure class="image is-48x48">
                                <img src="img/mila-square.png" alt="Mila Logo">
                            </figure>
                        </div>
                        <div class="media-content">
                            <p class="title is-4">Mila, University of Montreal</p>
                            <p class="subtitle is-6">PhD Candidate in Computer Science -
                                <time datetime="2022-09">September 2022</time>
                                -
                                <time datetime="2026-09">now</time>
                            </p>
                        </div>
                    </div>
                    <div class="content">
                        <p>Conducting research on real-time, interactive music accompaniment models using reinforcement
                            learning (RL) and multi-agent RL (MARL).</p>
                        <div class="tags">
                            <span class="tag">Interactive Music Generative Models</span>
                            <span class="tag">Reinforcement Learning</span>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card card-gap">
                <div class="card-content">
                    <div class="media">
                        <div class="media-left">
                            <figure class="image is-48x48">
                                <img src="img/Adobe-Logo-no-text.png" alt="Adobe Logo">
                            </figure>
                        </div>
                        <div class="media-content">
                            <p class="title is-4">Adobe Research, Co-Creation for Audio, Video, & Animation</p>
                            <p class="subtitle is-6">Student Researcher -
                                <time datetime="2024-06">June 2024</time>
                                -
                                <time datetime="2024-11">November 2024</time>
                            </p>
                        </div>
                    </div>
                    <div class="content">
                        <p>Researched open-vocabulary audio event localization techniques conditioned on text prompts.
                        </p>
                        <div class="tags">
                            <span class="tag">Multi-modal Representation Learning</span>
                            <span class="tag">Open-vocabulary sound event detection</span>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card card-gap">
                <div class="card-content">
                    <div class="media">
                        <div class="media-left">
                            <figure class="image is-48x48">
                                <img src="img/magenta-logo.png" alt="Google DeepMind Magenta Logo">
                            </figure>
                        </div>
                        <div class="media-content">
                            <p class="title is-4">Google DeepMind, Magenta Team</p>
                            <p class="subtitle is-6">Student Researcher -
                                <time datetime="2023-08">August 2023</time>
                                -
                                <time datetime="2024-05">May 2024</time>
                            </p>
                        </div>
                    </div>
                    <div class="content">
                        <p>Developed a reinforcement learning-based system, ReaLchords, for real-time melody-to-chord
                            accompaniment, and built GenJam, an interactive framework that enables delay-tolerant
                            inference and anticipatory output visualization.</p>
                        <div class="tags">
                            <span class="tag">Real-Time Music Interaction</span>
                            <span class="tag">Generative Models</span>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card card-gap">
                <div class="card-content">
                    <div class="media">
                        <div class="media-left">
                            <figure class="image is-48x48">
                                <img src="img/mila-square.png" alt="Mila Logo">
                            </figure>
                        </div>
                        <div class="media-content">
                            <p class="title is-4">Mila, University of Montreal</p>
                            <p class="subtitle is-6">CS Research Master -
                                <time datetime="2020-09">September 2020</time>
                                -
                                <time datetime="2022-09">September 2022</time>
                            </p>
                        </div>
                    </div>
                    <div class="content">
                        <p>Worked on hierarchical music generation models with detailed control.</p>
                        <ul>
                            <li>Proposed <a href="https://midi-ddsp.github.io/">MIDI-DDSP</a>, a model for controlling
                                musical performance and synthesis with hierarchical representation.</li>
                            <li>MIDI-DDSP enables high-fidelity audio reconstruction, accurate performance prediction,
                                and novel audio generation from note sequences.</li>
                        </ul>
                        <div class="tags">
                            <span class="tag">Hierarchical Music Models</span>
                            <span class="tag">Audio and Symbolic Music Generation</span>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card card-gap">
                <div class="card-content">
                    <div class="media">
                        <div class="media-left">
                            <figure class="image is-48x48">
                                <img src="img/tencent-ai-lab.png" alt="Tencent AI Lab Logo">
                            </figure>
                        </div>
                        <div class="media-content">
                            <p class="title is-4">Tencent AI Lab</p>
                            <p class="subtitle is-6">Research Intern -
                                <time datetime="2019-08">August 2019</time>
                                -
                                <time datetime="2020-05">May 2020</time>
                            </p>
                        </div>
                    </div>
                    <div class="content">
                        <p>Developed expressive singing voice synthesis methods and explored dynamic vocal modeling.</p>
                        <ul>
                            <li><a href="https://lukewys.github.io/files/Peking-Opera-Synthesis-2020.html">Peking Opera
                                    Synthesis</a>: Developed expressive singing synthesis using Peking Opera dynamics.
                            </li>
                            <li><a href="https://tencent-ailab.github.io/learning_singing_from_speech/">Learning Singing
                                    from Speech</a>: Created singing synthesis by fine-tuning speech synthesis with
                                pitch inputs.</li>
                        </ul>
                        <div class="tags">
                            <span class="tag">Singing Voice Synthesis</span>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card card-gap">
                <div class="card-content">
                    <div class="media">
                        <div class="media-left">
                            <figure class="image is-48x48">
                                <img src="img/bupt.png" alt="BUPT Logo">
                            </figure>
                        </div>
                        <div class="media-content">
                            <p class="title is-4">Beijing University of Posts and Telecommunications</p>
                            <p class="subtitle is-6">
                                <time datetime="2016-09">September 2016</time>
                                -
                                <time datetime="2020-06">June 2020</time>
                            </p>
                        </div>
                    </div>
                    <div class="content">
                        <p>Research focused on audio captioning, symbolic music datasets, and computational musicology.
                        </p>
                        <ul>
                            <li>Placed <a
                                    href="http://dcase.community/challenge2020/task-automatic-audio-captioning-results#wuyusong2020_t6">2nd</a>
                                in the DCASE 2020 Challenge for Automatic Audio Captioning.</li>
                            <li>Developed the <a href="https://github.com/lukewys/Guqin-Dataset">Chinese Guqin
                                    Dataset</a>, a symbolic music dataset.</li>
                            <li>Proposed statistical approaches to distinguish musical genres in computational
                                musicology.</li>
                        </ul>
                        <div class="tags">
                            <span class="tag">Music Generation</span>
                            <span class="tag">Automatic Audio Captioning</span>
                            <span class="tag">Computational Musicology</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container">
            <h1 class="title">Recent News</h1>
            <hr />
            <div class="content">
                <ul>
                    <il>Released <a href="https://github.com/lukewys/PianorollVis.js">PianorollVis.js</a>, a JavaScript
                        library for visualizing pianoroll.</il>
                    <li>Delivered a tutorial on <a href="https://github.com/lukewys/ISMIR2022-tutorial">designing
                            controllable synthesis systems for musical signals</a> at ISMIR 2022.
                    </li>

                </ul>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container">
            <h1 class="title">Contact</h1>
            <hr />
            <nav class="level">
                <div class="level-item has-text-centered">
                    <div>
                        <a href="https://github.com/lukewys" class="icon is-large">
                            <i class="fab fa-2x fa-github"></i>
                        </a>
                        <p class="heading"><a href="https://github.com/lukewys">github.com/lukewys</a></p>
                    </div>
                </div>
                <div class="level-item has-text-centered">
                    <div>
                        <a href="https://scholar.google.com/citations?user=vIuUJ-IAAAAJ&hl" class="icon is-large">
                            <i class="fas fa-2x fa-globe"></i>
                        </a>
                        <p class="heading"><a href="https://scholar.google.com/citations?user=vIuUJ-IAAAAJ&hl">Google
                                Scholar</a></p>
                    </div>
                </div>
                <div class="level-item has-text-centered">
                    <div>
                        <a href="mailto:wuyusongwys@gmail.com" class="icon is-large">
                            <i class="fas fa-2x fa-envelope"></i>
                        </a>
                        <p class="heading"><a href="mailto:wuyusongwys@gmail.com">wuyusongwys@gmail.com</a></p>
                    </div>
                </div>
                <div class="level-item has-text-centered">
                    <div>
                        <a href="https://www.linkedin.com/in/%E9%9B%A8%E6%9D%BE-%E5%90%B4-86b639187/"
                            class="icon is-large">
                            <i class="fab fa-2x fa-linkedin-in"></i>
                        </a>
                        <p class="heading"><a
                                href="https://www.linkedin.com/in/%E9%9B%A8%E6%9D%BE-%E5%90%B4-86b639187/">LinkedIn</a>
                        </p>
                    </div>
                </div>
            </nav>
        </div>
    </section>

    <!-- Modals -->
    <div id="project-1-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Hierarchical Music Generation with Detailed Control</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">

                <div id="project-1-modal-carousel" class="carousel">
                    <figure class="image carousel-cell">
                        <img src="img/midi-ddsp-ori-scale.png">
                    </figure>
                    <figure class="image carousel-cell">
                        <img src="img/midi-ddsp-listening-test.png">
                    </figure>
                    <figure class="image carousel-cell">
                        <img src="img/midi-ddsp-control-metric.png">
                    </figure>
                </div>

                <div class="content">

                    <li>Propose <a href="https://midi-ddsp.github.io/">MIDI-DDSP</a>, a hierarchical music generation
                        model
                        with explicit and interpretable representation for controlling musical performance and
                        synthesis.
                    </li>
                    <li>MIDI-DDSP can reconstruct high-fidelity audio,
                        accurately predict performance attributes for a note sequence,
                        independently manipulate the attributes of a given performance,
                        and as a complete system, generate realistic audio from a novel note sequence.
                    </li>
                    <p><a href="https://github.com/magenta/midi-ddsp">Code</a> |
                        <a href="https://magenta.tensorflow.org/midi-ddsp">Blog</a> |
                        <a
                            href="https://colab.research.google.com/github/magenta/midi-ddsp/blob/main/midi_ddsp/colab/MIDI_DDSP_Demo.ipynb">Colab
                            Notebook</a> |
                        <a href="https://huggingface.co/spaces/akhaliq/midi-ddsp">Huggingface Space</a> |
                        <a href="https://github.com/magenta/midi-ddsp#command-line-midi-synthesis">Command-line MIDI
                            Synthesis</a>
                    </p>


                    <div class="tags">
                        <span class="tag">Audio Synthesis</span>
                        <span class="tag">Generative Model</span>
                        <span class="tag">Hierarchical, DDSP</span>
                        <span class="tag">Structured Models</span>
                    </div>

                </div>

            </section>
            <footer class="modal-card-foot">
                <button class="button is-success">Close</button>
            </footer>
        </div>
    </div>

    <div id="project-2-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Automatic Audio Captioning with Transformer</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">

                <div id="project-2-modal-carousel" class="carousel">
                    <figure class="image carousel-cell">
                        <img src="img/dcase-ori-scale.png">
                    </figure>
                </div>

                <div class="content">

                    <li>Proposed a sequence-to-sequence model with a CNN as encoder and a Transformer as decoder,
                        with data augmentation, data regulation, pre-training and fine-tuning for accurate automatic
                        audio
                        captioning.
                    </li>
                    <li>The proposed system ranked
                        <a
                            href="http://dcase.community/challenge2020/task-automatic-audio-captioning-results#wuyusong2020_t6">2nd</a>
                        in all participants achieved by a SPIDEr score of 0.214.
                    </li>
                    <li>The proposed system won Reproducible System Award.</li>
                    <p><a
                            href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf">paper</a>
                    </p>


                    <div class="tags">
                        <span class="tag">Automatic Audio Captioning</span>
                        <span class="tag">Transformer</span>
                        <span class="tag">Seq-2-Seq Model</span>
                    </div>

                </div>

            </section>
            <footer class="modal-card-foot">
                <button class="button is-success">Close</button>
            </footer>
        </div>
    </div>

    <div id="project-3-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Expressive Peking Opera Synthesis</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div id="project-3-modal-carousel" class="carousel">
                    <figure class="image is-16by9 carousel-cell">
                        <img src="img/peking-opera-synthesis.png">
                    </figure>
                </div>

                <div class="content">
                    <p>Propose a Peking Opera singing synthesis system with expressiveness and dynamics in Peking opera
                        singing.
                        The model is trained by inputting musical note and learned from the spectrogram.</p>
                    <p><a href="https://lukewys.github.io/files/Peking-Opera-Synthesis-2020.html">demo</a>
                        <a href="https://lukewys.github.io/files/INTERSPEECH_2020_YusongWu_Final_revised.pdf">paper</a>
                    </p>
                    <div class="tags">
                        <span class="tag">Singing Voice Synthesis</span>
                    </div>
                </div>

            </section>
            <footer class="modal-card-foot">
                <button class="button is-success">Close</button>
            </footer>
        </div>
    </div>

    <div id="project-4-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Chinese Guqin Dataset</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div id="project-4-modal-carousel" class="carousel">
                    <figure class="image carousel-cell">
                        <img src="img/guqin.jpg">
                    </figure>
                </div>

                <div class="content">
                    <p>Collect and construct a symbolic music dataset of
                        <a href="https://github.com/lukewys/Guqin-Dataset">Chinese Guqin Dataset</a>.
                        Dataset is also used for proposing a statistical approach to distinguishing different music
                        genre.
                    </p>
                    <div class="tags">
                        <span class="tag">Computational Musicology</span>
                    </div>
                </div>

            </section>
            <footer class="modal-card-foot">
                <button class="button is-success">Close</button>
            </footer>
        </div>
    </div>

    <div id="project-5-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">3rd Place at AI Song Contest 2022</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div id="project-5-modal-carousel" class="carousel">
                    <figure class="image carousel-cell">
                        <img src="img/aisongcontest_atoi.jpeg">
                    </figure>
                </div>

                <div class="content">
                    <p>Together with Yuxuan Wu and Yi Deng, we created a song about the growth of the AI model from its
                        own
                        perspective using music generation models.
                        The song was ranked 3rd in the contest. To learn more about the song, please visit <a
                            href="https://www.aisongcontest.com/participants-2022/3i">here</a>.
                    </p>
                    <div class="tags">
                        <span class="tag">AI Song Contest</span>
                    </div>
                </div>

            </section>
            <footer class="modal-card-foot">
                <button class="button is-success">Close</button>
            </footer>
        </div>
    </div>
    <div id="project-6-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">MusicLDM: Enhancing Novelty in Text-to-Music
                    Generation Using Beat-Synchronous Mixup Strategies</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div id="project-6-modal-carousel" class="carousel">
                    <figure class="image carousel-cell">
                        <img src="img/musicldm_diagram.png">
                    </figure>
                </div>

                <div class="content">
                    <p>We propose MusicLDM, a text-to-music diffusion model that trains efficiently on limited dataset
                        and avoid plagiarism. We leverage a beat tracking model and propose two
                        different mixup strategies for data augmentation: beat-synchronous audio mixup
                        and beat-synchronous latent mixup. Such mixup strategies encourage the
                        model to interpolate between musical training samples and generate new music
                        within the convex hull of the training data, making the generated music more
                        diverse while still staying faithful to the corresponding style. To learn more:
                        <a href="https://musicldm.github.io/">sample page</a>, <a
                            href="https://github.com/RetroCirce/MusicLDM/">code</a>,<a
                            href="https://arxiv.org/pdf/2308.01546">paper</a>.
                    </p>
                    <div class="tags">
                        <span class="tag">text-to-music generation</span>
                        <span class="tag">multi-modal learning</span>
                    </div>
                </div>

            </section>
            <footer class="modal-card-foot">
                <button class="button is-success">Close</button>
            </footer>
        </div>
    </div>
    <div id="project-7-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">CLAP: large-scale contrastive language-audio model</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div id="project-7-modal-carousel" class="carousel">
                    <figure class="image carousel-cell">
                        <img src="img/clap_diagram.png">
                    </figure>
                </div>

                <div class="content">
                    <p>Together with Stability.ai, we created CLAP, a large-scale contrastive language-audio
                        representation learning model.
                        CLAP has been used in many projects and has been chosen widely be community for
                        standard feature extraction model to calculate Frechet Audio Distance. To learn more:
                        <a href="https://github.com/LAION-AI/CLAP">model github</a>, <a
                            href="https://github.com/LAION-AI/audio-dataset">data</a>,<a
                            href="https://arxiv.org/pdf/2211.06687">paper</a>.
                    </p>
                    <div class="tags">
                        <span class="tag">language-audio representation learning</span>
                        <span class="tag">multi-modal learning</span>
                    </div>
                </div>

            </section>
            <footer class="modal-card-foot">
                <button class="button is-success">Close</button>
            </footer>
        </div>
    </div>

    <div id="project-8-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">ReaLchords and GenJam: Real-time Melody-to-chord Accompaniment via RL</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div id="project-7-modal-carousel" class="carousel">
                    <figure class="image carousel-cell">
                        <img src="img/genjam_ui.png">
                    </figure>
                </div>

                <div class="content">
                    <p>We introduce MusicLDM, a text-to-music diffusion model designed to train efficiently on limited
                        datasets and avoid musical plagiarism. By leveraging beat-synchronous audio mixup strategies,
                        MusicLDM generates novel, stylistically faithful music. To
                        learn more:
                        <a href="https://storage.googleapis.com/realchords/index.html">ReaLchords webpage</a>,<a
                            href="https://arxiv.org/pdf/2211.06687">ReaLchords paper</a>, <a
                            href="https://storage.googleapis.com/genjam/index.html">GenJam webpage</a>.
                    </p>
                    <div class="tags">
                        <span class="tag">Real-time music accompaniment</span>
                        <span class="tag">Interactive music generation</span>
                    </div>
                </div>

            </section>
            <footer class="modal-card-foot">
                <button class="button is-success">Close</button>
            </footer>
        </div>
    </div>
    <script src="js/bundle.js"></script>
</body>

</html>