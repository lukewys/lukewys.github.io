<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.55.5" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="css/normalize.css">
<link rel="stylesheet" href="css/skeleton.css">
<link rel="stylesheet" href="css/custom.css">
<title>Audio Sample of &#34;Synthesising Expressiveness in Peking Opera via Duration Informed Attention Network&#34; </title>
</head>

<body>

<div class="container">

	<header role="banner">




	</header>


    <main role="main">

<h1 id="title">Synthesising Expressiveness in Peking Opera</h1>

<p>This paper presents a method that generates expressive singing voice of Peking opera. The synthesis of expressive opera singing usually requires pitch contours to be extracted as the training data, which relies on techniques and is not able to be manually labeled. With the Duration Informed Attention Network (DurIAN), this paper makes use of musical note instead of pitch contours for expressive opera singing synthesis. The proposed method enables human annotation being combined with automatic extracted features to be used as training data thus the proposed method gives extra flexibility in data collection for Peking opera singing synthesis. Comparing with the expressive singing voice of Peking opera synthesised by pitch contour based system, the proposed musical note based system produces comparable singing voice in Peking opera with expressiveness in various aspects.</p>
<p>In short: the model generates <b>singing voice</b> of <b>Peking Opera</b> from note and phoneme sequence where <b>pitch, dynamics and timbre</b> are jointly sampled.</p>

<h2 id="methods">How it works?</h2>
        <ul>
  <li>A <b>phoneme</b> encoder first encode contextual phoneme features.</li>
  <li>Then <b>alignment model</b> is used to add note information as well as other conditions (singer, role type) and align all feature sequence with the output frame.</li>
  <li><b>Auto-regressive decoder</b> is used to generate <b>spectrogram</b>.</li>
  <li><b>Griffin-Lim</b> or neural <b>vocoder</b> such as <b>WaveRNN</b> is used to generate audio signal from spectrogram.</li>
</ul>
<img src="ISMIR2019-audio-sample/image/Peking Opera Synthesis.bmp" alt="Model Architecture">

<h2 id="audio-samples">Audio Samples</h2>

<p>f0-system</p>

<table><thead><tr>
<th style="text-align: center">200-Pairs Only</th>
<th style="text-align: center">Our Method</th>
</tr></thead><tbody><tr>
<td style="text-align: center"><audio controls="controls" ><source src="ISMIR2019-audio-sample/f0/f0_Convert_female_lseh-Wo_ben_shi-Qiong_lin_yan-qm-1.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="audio/unsuper/2.wav" autoplay/>Your browser does not support the audio element.</audio> </td>
</tr></tbody></table>

<table><thead><tr>
<th style="text-align: center">200-Pairs Only</th>
<th style="text-align: center">Our Method</th>
</tr></thead><tbody><tr>
<td style="text-align: center"><audio controls="controls" ><source src="ISMIR2019-audio-sample/f0/f0_Convert_female_lseh-Wo_ben_shi-Qiong_lin_yan-qm-1.wav" autoplay/>Your browser does not support the audio element.</audio></td>
<td style="text-align: center"><audio controls="controls" ><source src="audio/unsuper/2.wav" autoplay/>Your browser does not support the audio element.</audio> </td>
</tr></tbody></table>



    </main>
    </div>

</body>
</html>